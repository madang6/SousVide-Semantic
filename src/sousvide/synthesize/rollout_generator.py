import numpy as np
import json
import pickle
import yaml
import os
import copy
import torch
import re

from scipy.spatial.transform import Rotation as R, Slerp

from typing import Dict,Union,Tuple,List,Optional
from tqdm.notebook import trange
from tqdm import tqdm

import figs.utilities.trajectory_helper as th
import figs.tsplines.min_snap as ms
import figs.tsampling.build_rrt_dataset as bd
import sousvide.synthesize.synthesize_helper as sh
import sousvide.synthesize.data_utils as du
import sousvide.flight.vision_preprocess_alternate as vp

import sousvide.flight.deploy_ssv as df
from figs.simulator import Simulator
from figs.control.vehicle_rate_mpc import VehicleRateMPC
from figs.dynamics.model_specifications import generate_specifications

def generate_rollout_data(cohort_name:str,method_name:str,
                          flights:List[Tuple[str,str]],
                          Nro_sv:int=50,
                          validation_mode:bool=False):
    
    """
    Generates flight data for a given cohort. A cohort comprises a set of courses flown on a specific
    drone frame with a specific method of domain randomization. Flight data is agnostic to the pilot 
    and is generated by simulating variations of the drone over the set of courses using an MPC flight
    controller that has full knowledge. The flight data is saved to a .pt file in the cohort directory.

    Args:
        cohort_name:    Cohort name.
        method_name:    Sous Vide config.
#FIXME  flights:        List of flights ??(scene,frame,course)??. (scene, course)
        Nro_sv:         Number of rollouts per save.

    Returns:
        None:           (flight data saved to cohort directory)
    """

    # Some useful path(s)
    workspace_path = os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    
    # Extract method configs
    method_path = os.path.join(workspace_path,"configs","method",method_name+".json")

    # Extract scene configs
    scenes_cfg_dir  = os.path.join(workspace_path, "configs", "scenes")

    # Set course config path
    course_cfg_path = os.path.join(workspace_path, "configs", "course")
        
    with open(method_path) as json_file:
        method_config = json.load(json_file)
    
    sample_set_config = method_config["sample_set"]
    trajectory_set_config = method_config["trajectory_set"]
    loiter_set_config = method_config["loiter_set"]
    frame_set_config = method_config["frame_set"]
    test_set_config = method_config["test_set"]

    rrt_mode = sample_set_config["rrt_mode"]
    loitering = sample_set_config["loitering"]
    Tdt_ro = sample_set_config["duration"]
    Nro_tp = sample_set_config["reps"]
    Trep   = np.zeros(Nro_tp)
    Ntp_sc = sample_set_config["rate"]
    err_tol = sample_set_config["tolerance"]
    rollout_name = sample_set_config["rollout"]
    policy_name = sample_set_config["policy"]
    frame_name = sample_set_config["frame"]
    use_clip   = sample_set_config["clipseg"] or test_set_config["clipseg"]

    # Extract policy and frame
    policy_path = os.path.join(workspace_path,"configs","policy",policy_name+".json")
    frame_path  = os.path.join(workspace_path,"configs","frame",frame_name+".json")
    
    with open(policy_path) as json_file:
        policy_config = json.load(json_file)

    with open(frame_path) as json_file:
        base_frame_config = json.load(json_file)   

    hz_ctl = policy_config["hz"]

    # Create cohort folder
    cohort_path = os.path.join(workspace_path,"cohorts",cohort_name)

    if not os.path.exists(cohort_path):
        os.makedirs(cohort_path)

    # Generate base drone specifications
    base_frame_specs = generate_specifications(base_frame_config)

    # Print some useful information
    print("==========================================================================")
    print("Cohort         :",cohort_name)
    print("Method         :",method_name)
    print("Policy         :",policy_name)
    print("Frame          :",frame_name)
    print("Flights        :",flights)
    # print("Trajectory Type:","")

    if use_clip:
        print("Using CLIPSeg for semantic segmentation.")
        vision_processor = vp.CLIPSegHFModel(
            hf_model="CIDAS/clipseg-rd64-refined"
        )
    else:
        vision_processor = None

    if rrt_mode and not validation_mode:
        for scene_name,course_name in flights:
            scene_cfg_file = os.path.join(scenes_cfg_dir, f"{scene_name}.yml")
            combined_prefix = os.path.join(scenes_cfg_dir, scene_name)
            with open(scene_cfg_file) as f:
                scene_cfg = yaml.safe_load(f)

            objectives      = scene_cfg["queries"]
            radii           = scene_cfg["radii"]
            n_branches      = scene_cfg["nbranches"]
            hover_mode      = scene_cfg["hoverMode"]
            visualize_flag = scene_cfg["visualize"]
            altitudes       = scene_cfg["altitudes"]
            similarities    = scene_cfg.get("similarities", None)
            num_trajectories = scene_cfg.get("numTraj", "all")
            n_iter_rrt = scene_cfg["N"]
            env_bounds      = {}
            if "minbound" in scene_cfg and "maxbound" in scene_cfg:
                env_bounds["minbound"] = np.array(scene_cfg["minbound"])
                env_bounds["maxbound"] = np.array(scene_cfg["maxbound"])

            # Generate simulator
            simulator = Simulator(scene_name,rollout_name)

            # RRT-based trajectories
            obj_targets, _, epcds_list, epcds_arr = bd.get_objectives(
                simulator.gsplat, objectives, similarities, visualize_flag
            )

            # Goal poses and centroids
            goal_poses, obj_centroids = th.process_RRT_objectives(
                obj_targets, epcds_arr, env_bounds, radii, altitudes
            )

            # Obstacle centroids and rings
            rings, obstacles = th.process_obstacle_clusters_and_sample(
                epcds_arr, env_bounds)
            
            print(f"obstacles poses : {obstacles}")
            print(f"rings poses shape: {len(rings)}")

            # Generate RRT paths
            raw_rrt_paths = bd.generate_rrt_paths(
                scene_cfg_file, simulator, epcds_list, epcds_arr, objectives,
                goal_poses, obj_centroids, env_bounds, rings, obstacles, n_iter_rrt
            )

            print("==== OBJECTIVES ====")
            print("objectives:", objectives)
            print("raw_rrt_paths keys:", list(raw_rrt_paths.keys()))
            print("====================")

            # Filter and parameterize trajectories
            all_trajectories = {}
            raw_filtered = {}
            for idx, obj_name in enumerate(objectives):
                branches = raw_rrt_paths[obj_name]
                alt_set  = th.set_RRT_altitude(branches, altitudes[idx])
                filtered = th.filter_branches(alt_set, n_branches[idx], hover_mode)
                raw_filtered[obj_name] = filtered
                print(f"{obj_name}: {len(filtered)} branches")
                # print(f"filtered branches: {filtered}")

                all_trajectories[obj_name], _ = th.parameterize_RRT_trajectories(
                    filtered, obj_centroids[idx], 1.0, 20
                )
#FIXME
            # if loitering:
            #     loiter_trajectories = {}
            #     for idx in range(len(obstacles)):
            #         print(f"Rings for loiter: {[rings[idx]]}")
            #         all_trajectories[f"loiter_{idx}"], _ = th.parameterize_RRT_trajectories(
            #             [rings[idx]], obstacles[idx], constant_velocity=1.0, sampling_frequency=20, loiter=True)
            #     objectives.extend([f"null" for _ in range(len(obstacles))])

                    # print(f"Parameterized: {len(traj_list)} loiter trajectories for obstacle {idx}")
                # print(f"chosen_traj.shape: {traj_list[0].shape}")
                # combined_data = {
                #     "tXUi": traj_list[0],
                #     "nodes": node_list[0],
                #     **debug_info
                # }
                # combined_file = f"{combined_prefix}_loiter_{idx}.pkl"
                # with open(combined_file, "wb") as f:
                #     pickle.dump(combined_data, f)
                # trajectory_dataset[f"loiter_{idx}"] = combined_data

            # Plot filtered trajectories
            bd.visualize_rrt_trajectories(
                raw_filtered,
                scene_cfg_file, simulator, epcds_list, epcds_arr, objectives,
                goal_poses, obj_centroids, env_bounds, rings, obstacles
            )
#FIXME
            print("Generating loitering trajectories...")
            if loitering:
                rollout_loiter(
                    all_trajectories, objectives,
                    Tdt_ro, base_frame_specs, course_cfg_path,
                    combined_prefix, cohort_name, method_name, flights
                )
#             orig_courses = list(all_trajectories.keys())
#             orig_name = dict(zip(orig_courses, objectives))
#             loiter_id = 0
#             if loitering:
#                 total_batches = int(sum(
#                 np.ceil((Nro_tp * (int(tXUi[0,-1]) // int(Tdt_ro))) / Nro_sv)
#                 for course in orig_courses
#                 for tXUi in all_trajectories[course]
#                 ))
#                 batch_pbar = tqdm(total=total_batches, desc="Solving Loiter Batches")

#                 for course_idx, course in enumerate(orig_courses):
#                     trajectories = all_trajectories[course]
#                     loiter_key = f"loiter_{course}"
#                     all_trajectories[loiter_key] = []
#                     loiter_trajectories = []

#                     for traj_idx, tXUi in enumerate(trajectories):
#                         duration = int(tXUi[0][-1])
#                         splits = duration // int(Tdt_ro)
#                         n_time = splits
#                         n_samples = Nro_tp * n_time
#                         times = np.tile(
#                             np.linspace(tXUi[0][0], tXUi[0][-1], n_time + 1)[:-1],
#                             Nro_tp
#                         )
#                         times += np.random.uniform(
#                             -1 / Ntp_sc,
#                             1 / Ntp_sc,
#                             n_samples
#                         )
#                         times = np.clip(times, tXUi[0][0], tXUi[0][-1])
#                         np.random.shuffle(times)
#                         batches = np.split(
#                             times,
#                             np.arange(Nro_sv, n_samples, Nro_sv)
#                         )

#                         for batch_idx, batch_times in enumerate(batches):
#                             loiter_tXUd = generate_loiter_trajectories(
#                                 tXUd_rrt=tXUi, Tpd = tXUi[0], Tsps=Trep, base_frame_specs=base_frame_specs,
#                                 course_cfg_path=course_cfg_path
#                             )
#                             batch_pbar.update()
#                             loiter_trajectories.extend(loiter_tXUd)
# #FIXME verifies we're doing
# #      the right thing here
#                             if loiter_id == 0:
#                                 print(f"Simulating loitering trajectory for course '{course}'.")
#                                 combined_data = {
#                                     "tXUi": loiter_tXUd[0]
#                                 }
#                                 combined_file = f"{combined_prefix}_verify_loiter_{course}.pkl"
#                                 with open(combined_file, "wb") as f:
#                                     pickle.dump(combined_data, f)

#                                 import sousvide.flight.deploy_ssv as df
#                                 df.simulate_roster(cohort_name=cohort_name,
#                                                    method_name=method_name,
#                                                    flights=flights,
#                                                    roster=[],
#                                                    review=True,
#                                                    filename=combined_file
#                                                    )
#                             loiter_id += 1
#                     all_trajectories[loiter_key].extend(loiter_trajectories)
#                 batch_pbar.close()

            # Simulate and save rollouts
            print("Simulating and saving rollouts...")
            rollout_trajectories(
                all_trajectories, dict(zip(list(all_trajectories.keys()), objectives)), Nro_tp,
                sample_set_config, Tdt_ro, err_tol, Ntp_sc, Nro_sv,
                Trep, base_frame_config, frame_set_config,
                trajectory_set_config, loiter_set_config,
                simulator, policy_config,
                vision_processor,
                cohort_path, course_name
            )
            # stack_id = 0
            # batch_pbar = tqdm(total=total_batches, desc="Rollout Data Batches")
            # for course_idx, course in tqdm(enumerate(all_trajectories),desc=f"Objects"):
            #     if course.startswith("loiter_"):
            #         base = course[len("loiter_"):]
            #         is_loiter = True
            #     else:
            #         base = course
            #         is_loiter = False
            #     trajectories = all_trajectories[course]
            #     total = Nro_tp * len(trajectories)
            #     print(f"Preparing {total} samples for course '{course}', query: {orig_name[base]}")
            #     if course.startswith("loiter_"):
            #         Nro_tp = sample_set_config["reps"]
            #     else:
            #         Nro_tp = sample_set_config["reps"]

            #     for traj_idx, tXUi in enumerate(trajectories):                
            #         duration = int(tXUi[0][-1])
            #         splits = duration // int(Tdt_ro)
            #         n_time = splits

            #         n_samples = Nro_tp * n_time

            #         times = np.tile(
            #             np.linspace(tXUi[0][0], tXUi[0][-1], n_time + 1)[:-1],
            #             Nro_tp
            #         )
            #         times += np.random.uniform(
            #             -1 / Ntp_sc,
            #             1 / Ntp_sc,
            #             n_samples
            #         )
            #         times = np.clip(times, tXUi[0][0], tXUi[0][-1])
            #         np.random.shuffle(times)
            #         batches = np.split(
            #             times,
            #             np.arange(Nro_sv, n_samples, Nro_sv)
            #         )

            #         data_count = 0
            #         for batch_idx, batch_times in enumerate(batches):
            #             if course.startswith("loiter_"):
            #                 Frames = generate_frames(
            #                     Tsps=Trep, base_frame_config=base_frame_config, frame_set_config=frame_set_config
            #                 )
            #                 Perturbations = generate_perturbations(
            #                     Tsps=Trep, tXUi=tXUi, trajectory_set_config=loiter_set_config
            #                 )
            #             else:
            #                 Frames = generate_frames(
            #                     Tsps=Trep, base_frame_config=base_frame_config, frame_set_config=frame_set_config
            #                 )
            #                 Perturbations = generate_perturbations(
            #                     Tsps=Trep, tXUi=tXUi, trajectory_set_config=trajectory_set_config
            #                 )

            #             Trajectories,Images,Img_data = generate_rollouts(
            #             simulator, sample_set_config,
            #             tXUd=tXUi, 
            #             objective=orig_name[base],
            #             policy_config=policy_config,
            #             Frames=Frames,
            #             Perturbations=Perturbations,
            #             Tdt_ro=Tdt_ro, err_tol=err_tol,
            #             vision_processor=vision_processor,
            #             )

            #             save_rollouts(
            #                 cohort_path, course_name, 
            #                 Trajectories, Images, Img_data, 
            #                 tXUi, stack_id
            #             )
            #             stack_id += 1
            #             data_count += sum(r["Ndata"] for r in Trajectories)
            #             batch_pbar.update()

            #         print(f"Trajectory {traj_idx} generated {data_count} data points")
            # batch_pbar.close()
            
    elif validation_mode:
        Nro_tp = 1
        for scene_name,course_name in flights:
            scene_cfg_file = os.path.join(scenes_cfg_dir, f"{scene_name}.yml")
            combined_prefix = os.path.join(scenes_cfg_dir, scene_name)
            with open(scene_cfg_file) as f:
                scene_cfg = yaml.safe_load(f)

            objectives      = scene_cfg["queries"]
            radii           = scene_cfg["radii"]
            if loitering:
                n_branches = [nb // 10 for nb in scene_cfg["nbranches"]]
                # n_branches = [1, 1, 1]
            else:
                n_branches = [nb // 10 for nb in scene_cfg["nbranches"]]
                
            print(f"n_branches: {n_branches}")
            hover_mode      = scene_cfg["hoverMode"]
            visualize_flag = scene_cfg["visualize"]
            altitudes       = scene_cfg["altitudes"]
            similarities    = scene_cfg.get("similarities", None)
            num_trajectories = scene_cfg.get("numTraj", "all")
            n_iter_rrt = scene_cfg["N"]
            env_bounds      = {}
            if "minbound" in scene_cfg and "maxbound" in scene_cfg:
                env_bounds["minbound"] = np.array(scene_cfg["minbound"])
                env_bounds["maxbound"] = np.array(scene_cfg["maxbound"])

            # Generate simulator
            simulator = Simulator(scene_name,rollout_name)

            # RRT-based trajectories
            obj_targets, _, epcds_list, epcds_arr = bd.get_objectives(
                simulator.gsplat, objectives, similarities, visualize_flag
            )

            # Goal poses and centroids
            goal_poses, obj_centroids = th.process_RRT_objectives(
                obj_targets, epcds_arr, env_bounds, radii, altitudes
            )

            # Obstacle centroids and rings
            rings, obstacles = th.process_obstacle_clusters_and_sample(
                epcds_arr, env_bounds)
            
            print(f"obstacles poses : {obstacles}")
            print(f"rings poses shape: {len(rings)}")

            # Generate RRT paths
            raw_rrt_paths = bd.generate_rrt_paths(
                scene_cfg_file, simulator, epcds_list, epcds_arr, objectives,
                goal_poses, obj_centroids, env_bounds, rings, obstacles, n_iter_rrt
            )

            print("==== OBJECTIVES ====")
            print("objectives:", objectives)
            print("raw_rrt_paths keys:", list(raw_rrt_paths.keys()))
            print("====================")

            # Filter and parameterize trajectories
            all_trajectories = {}
            raw_filtered = {}
            for idx, obj_name in enumerate(objectives):
                branches = raw_rrt_paths[obj_name]
                alt_set  = th.set_RRT_altitude(branches, altitudes[idx])
                filtered = th.filter_branches(alt_set, n_branches[idx], hover_mode)
                raw_filtered[obj_name] = filtered
                print(f"{obj_name}: {len(filtered)} branches")

                all_trajectories[obj_name], _ = th.parameterize_RRT_trajectories(
                    filtered, obj_centroids[idx], 1.0, 20
                )
#FIXME            
            # if loitering:
            #     loiter_trajectories = {}
            #     for idx in range(len(obstacles)):
            #         print(f"Rings for loiter: {[rings[idx]]}")
            #         all_trajectories[f"loiter_{idx}"], _ = th.parameterize_RRT_trajectories(
            #             [rings[idx]], obstacles[idx], 1.0, 20, loiter=True)
            #     objectives.extend([f"null" for _ in range(len(obstacles))])
#
            # Plot filtered trajectories
            bd.visualize_rrt_trajectories(
                raw_filtered,
                scene_cfg_file, simulator, epcds_list, epcds_arr, objectives,
                goal_poses, obj_centroids, env_bounds, rings, obstacles
            )
#FIXME
            print("Generating loitering trajectories...")
            if loitering:
                rollout_loiter(
                    all_trajectories, objectives,
                    Tdt_ro, base_frame_specs, course_cfg_path,
                    combined_prefix, cohort_name, method_name, flights, validation_mode=validation_mode
                )
#
            # Simulate and save rollouts
            print("Simulating and saving rollouts...")
            rollout_trajectories(
                all_trajectories=all_trajectories,
                objectives=dict(zip(list(all_trajectories.keys()), objectives)),
                Nro_tp=Nro_tp,
                sample_set_config=sample_set_config,
                Tdt_ro=Tdt_ro,
                err_tol=err_tol,                   
                Ntp_sc=Ntp_sc,
                Nro_sv=Nro_sv,
                Trep=Trep,
                base_frame_config=base_frame_config,
                frame_set_config=frame_set_config,
                trajectory_set_config=trajectory_set_config,
                loiter_set_config=loiter_set_config,
                simulator=simulator,
                policy_config=policy_config,
                vision_processor=vision_processor,
                cohort_path=cohort_path,
                course_name=course_name,
                validation_mode=validation_mode
                )

    else:   
        # Generate rollouts for each course
        for scene_name,course_name in flights:
            # Load course_config
            course_path = os.path.join(workspace_path,"configs","course",course_name+".json")

            with open(course_path) as json_file:
                course_config = json.load(json_file)
            
            # Add course name
            course_config["name"] = course_name

            # Generate desired trajectory
            output = ms.solve(course_config)
            if output is not False:
                Tpd,CPd = output
                tXUd = th.TS_to_tXU(Tpd,CPd,base_frame_specs,hz_ctl)
            else:
                raise ValueError("Desired trajectory not feasible. Aborting.")
            
            # Generate simulator
            simulator = Simulator(scene_name,rollout_name)
            
            # Generate Sample Set Batches
            Ntp = Ntp_sc*int(Tpd[-1])                                       # Number of time points per trajectory
            Nsp = Nro_tp*Ntp                                                # Number of sample points (total)
            
            Tsp = np.tile(np.linspace(Tpd[0],Tpd[-1],Ntp+1)[:-1],Nro_tp)    # Entire sample points array
            Tsp += np.random.uniform(-1/Ntp_sc,1/Ntp_sc,Nsp)                # Add some noise to the sample points array
            Tsp = np.clip(Tsp,Tpd[0],Tpd[-1])                               # Clip the sample points array
            np.random.shuffle(Tsp)                                          # Shuffle the sample points array

            TTsp = np.split(Tsp,np.arange(Nro_sv,Nsp,Nro_sv))               # Split the sample points array into their batches
            
            # Print some diagnostics
            Ndc = int(Tdt_ro*hz_ctl)

            print("--------------------------------------------------------------------------")
            print("Course Name :",course_name)
            print("Rollout Reps:",Nro_tp,"(per time point)")
            print("Rollout Rate:",Ntp_sc,"(per second)")
            print("Rollout Data:",Ndc,"(per sample)")
            print("--------------------------------------------------------------------------")
            print("Total Rollouts:",Nsp)
            print("Batch Sizes :", len(TTsp)-1, "x", Nro_sv,"+ 1 x", len(TTsp[-1]))
            print("Total Data:",Nsp*Ndc)
            print("--------------------------------------------------------------------------")

            # Generate Sample Set Batches
            Ndata = 0
            for idx in trange(len(TTsp)):
                # Get the current batch
                Tsp = TTsp[idx]
                
                # Generate sample frames
                Frames = generate_frames(Tsp,base_frame_config,frame_set_config)

                # Generate sample perturbations
                Perturbations  = generate_perturbations(Tsp,Tpd,CPd,trajectory_set_config)

                # Generate rollout data
                Trajectories,Images = generate_rollouts(
                    simulator,course_config,policy_config,
                    Frames,Perturbations,Tdt_ro,err_tol)

                # Save the rollout data
                save_rollouts(cohort_path,course_name,Trajectories,Images,tXUd,idx)

                # Update the data count
                Ndata += sum([trajectory["Ndata"] for trajectory in Trajectories])

            # Print some diagnostics
            print("--------------------------------------------------------------------------")
            print("Generated ",Ndata," points of data.")
            print("--------------------------------------------------------------------------")

def compute_batches(tXUi, reps, Tdt_ro, Ntp_sc, Nro_sv, validation_mode=False):
    """
    Given a trajectory tXUi and sampling parameters, return a list of
    arrays of sample times (each of length ≤ Nro_sv).
    """
    t0, t1 = tXUi[0][0], tXUi[0][-1]
    duration = int(t1 - t0)
    if validation_mode:
        n_intervals = 1
    else:
        n_intervals = max(1, int((t1 - t0) // Tdt_ro))
    n_samples = reps * n_intervals

    # uniform grid then jitter, clip, shuffle, split
    times = np.tile(np.linspace(t0, t1, n_intervals + 1)[:-1], reps)
    times += np.random.uniform(-1 / Ntp_sc, 1 / Ntp_sc, n_samples)
    times = np.clip(times, t0, t1)
    np.random.shuffle(times)
    return np.split(times, np.arange(Nro_sv, n_samples, Nro_sv))

def compute_intervals(tXUi, Tdt_ro, validation_mode=False):
    """
    Compute the start times of each coarse interval for loiter spins.
    Returns an array of times (one per interval).
    """
    t0, t1     = tXUi[0, 0], tXUi[0, -1]
    if validation_mode:
        n_intervals = 1
    else:
        n_intervals = max(1, int((t1 - t0) // Tdt_ro))
    # intervals at multiples of Tdt_ro
    return t0 + np.arange(n_intervals) * Tdt_ro

def rollout_loiter(all_trajectories, objectives,
                   Tdt_ro, base_frame_specs, course_cfg_path,
                   combined_prefix, cohort_name, method_name, flights, validation_mode=False):
    orig_courses = list(all_trajectories.keys())
    orig_name = dict(zip(orig_courses, objectives))
    total_intervals = sum(
        len(compute_intervals(tXUi, Tdt_ro))
        for course in orig_courses
        for tXUi in all_trajectories[course]
    )
    # total_batches = sum(
    #     len(compute_batches(tXUi, Nro_tp, Tdt_ro, Ntp_sc, Nro_sv, validation_mode=validation_mode))
    #     for c in orig_courses
    #     for tXUi in all_trajectories[c]
    # )

    loiter_id = 0
    with tqdm(total=total_intervals, desc="Solving Loiter Intervals") as pbar:
        for course in orig_courses:
            loiter_key = f"loiter_{course}"
            loiter_list = []
            for tXUi in all_trajectories[course]:
                # for _batch in compute_batches(tXUi, Nro_tp, Tdt_ro, Ntp_sc, Nro_sv, validation_mode=validation_mode):
                Tsps = compute_intervals(tXUi, Tdt_ro, validation_mode=validation_mode)
                loiter_tXUd = generate_loiter_trajectories(
                    tXUd_rrt         = tXUi,
                    Tpd              = tXUi[0],
                    Tsps             = Tsps,
                    base_frame_specs = base_frame_specs,
                    course_cfg_path  = course_cfg_path
                )
                pbar.update(len(loiter_tXUd))
                loiter_list.extend(loiter_tXUd)

                # one‐time verify dump + simulate
                if loiter_id == 0:
                    print(f"Simulating loitering trajectory for course '{course}'.")
                    randix = np.random.randint(0, len(loiter_tXUd))
                    combined_data = {"tXUi": loiter_tXUd[randix]}
                    fname = f"{combined_prefix}_verify_loiter_{course}.pkl"
                    with open(fname, "wb") as f:
                        pickle.dump(combined_data, f)
                    df.simulate_roster(
                        cohort_name=cohort_name,
                        method_name=method_name,
                        flights=flights,
                        roster=[],
                        review=True,
                        filename=fname
                    )
                loiter_id += 1
            
            all_trajectories[loiter_key] = loiter_list

def rollout_trajectories(all_trajectories, objectives, Nro_tp,
                sample_set_config, Tdt_ro, err_tol, Ntp_sc, Nro_sv,
                Trep, base_frame_config, frame_set_config,
                trajectory_set_config, loiter_set_config,
                simulator, policy_config,
                vision_processor,
                cohort_path, course_name,
                validation_mode=False):

    orig_courses = [c.replace("loiter_", "") for c in objectives.keys()]
    orig_name = dict(zip(orig_courses, objectives))

    total_batches = sum(
        len(compute_batches(tXUi,
                            Nro_tp,
                            Tdt_ro, Ntp_sc, Nro_sv, validation_mode=validation_mode))
        for c in all_trajectories
        for tXUi in all_trajectories[c]
    )
    stack_id = 0

    with tqdm(total=total_batches, desc="Rollout Data Batches") as pbar:
        for course in all_trajectories:
            base = course[len("loiter_"):] if course.startswith("loiter_") else course
            is_loiter = course.startswith("loiter_")
            reps = Nro_tp

            print(f"Preparing {reps * len(all_trajectories[course])} samples "
                  f"for course '{course}', query: {orig_name[base]}")

            for traj_idx, tXUi in enumerate(all_trajectories[course]):
                for _batch in compute_batches(tXUi, reps, Tdt_ro, Ntp_sc, Nro_sv, validation_mode=validation_mode):
                    # shared frame generation
                    Frames = generate_frames(
                        Tsps=Trep,
                        base_frame_config=base_frame_config,
                        frame_set_config=frame_set_config
                    )
                    # pick correct perturbation config
                    cfg = loiter_set_config if is_loiter else trajectory_set_config
                    Perturbations = generate_perturbations(
                        Tsps=Trep,
                        tXUi=tXUi,
                        trajectory_set_config=cfg
                    )

                    # actual rollout + save
                    Trajectories, Images, Img_data = generate_rollouts(
                        simulator, sample_set_config,
                        tXUd=tXUi,
                        objective=orig_name[base],
                        policy_config=policy_config,
                        Frames=Frames,
                        Perturbations=Perturbations,
                        Tdt_ro=Tdt_ro,
                        err_tol=err_tol,
                        vision_processor=vision_processor,
                        validation_mode=validation_mode
                    )
                    save_rollouts(
                        cohort_path, course_name,
                        Trajectories, Images, Img_data,
                        tXUi, stack_id,
                        validation_mode=validation_mode
                    )
                    stack_id += 1
                    pbar.update()

    print("All rollouts complete.")

def generate_frames(Tsps:np.ndarray,
                    base_frame_config:Dict[str,Union[int,float,List[float]]],
                    frame_set_config:Dict[str,Union[float,List[float]]],
                    rng_seed:Union[int,None]=None) -> List[Dict[str,Union[np.ndarray,str,int,float]]]:
    
    """
    Generates a list of drone variations for a given base drone configuration. The configurations are
    generated by perturbing the base drone configuration with bounded uniform noise. The number of
    configurations generated is determined by the sample set config dictionary.

    Args:
        Tsps:                   Sample times.
        base_frame_config:      Base frame configuration dictionary.
        frame_set_config:       Frame sample set config dictionary.
        rng_seed:               Random number generator seed.

    Returns:
        Drones:                 List of drone configurations (dictionary format).
    """

    # Sample Count
    Nsps = len(Tsps)

    # Set random number generator seed
    if rng_seed is not None:
        np.random.seed(rng_seed)

    # Get frame items that will be randomized
    Items = {}
    for key in frame_set_config.keys():
        if key not in base_frame_config.keys():
            raise ValueError(f"Key '{key}' not found in base frame config.")
        else:
            value = np.array(base_frame_config[key])
            bound = np.array(frame_set_config[key])

            Items[key] = [value*(1-bound),value*(1+bound)]

    # Generate Drone Frames
    Frames = []
    for _ in range(Nsps):
        # Instantiate a new frame
        frame = copy.deepcopy(base_frame_config)

        # Randomize the frame
        for key in Items.keys():
            frame[key] = np.random.uniform(*Items[key])

        # Save to a dictionary
        Frames.append(frame)

    return Frames

def generate_loiter_trajectories(tXUd_rrt: np.ndarray,
                                 Tpd: np.ndarray,
                                 Tsps: np.ndarray,
                                 base_frame_specs,
                                 course_cfg_path: str = None,
                                 spin_duration: float = 4.0,
                                 smooth_duration: float = 1.0,
                                 Nco: int = 6,
                                 hz: int = 20,
                                 simulate: bool = False,
                                 ) -> List[np.ndarray]:
    """
    For each sample time in Tsps, splice in a constant-rate spin
    at the RRT state at that time, then smoothly blend
    back to the RRT state before returning the loiter segment tXUd.

    Returns:
      loiter_trajs
    """
    def quat_to_yaw(q):
        x, y, z, w = q
        return np.arctan2(2*(w*z + x*y), 1 - 2*(y*y + z*z))
    
    loiter_trajs = []

    idx = 0
    for t0 in Tsps:
        # 1) find index in RRT trajectory
        idx0 = np.searchsorted(Tpd, t0) - 1
        idx0 = np.clip(idx0, 0, len(Tpd)-2)

        # 2) pull out the XYZ, velocity, quaternion
        pos_rrt = tXUd_rrt[1:4, idx0]   # x,y,z
        vel_rrt = tXUd_rrt[4:7, idx0]   # dx,dy,dz
        q_rrt   = tXUd_rrt[7:11, idx0]  # quaternion

        # 3) pick a random start yaw, and the RRT yaw as final
        yaw0    = np.random.uniform(0, 2*np.pi)
        # yaw0 = quat_to_yaw(q_rrt)
        yaw1    = quat_to_yaw(q_rrt)

        # dθ        = ((yaw1 - yaw0 + np.pi) % (2*np.pi)) - np.pi
        # direction = np.sign(dθ) if dθ != 0 else 1.0
        # abs_dθ    = abs(dθ)
        # total_ang = abs_dθ + 2*np.pi

        # # 3) your expected θ_end:
        # theta_end = yaw0 + direction * total_ang

        # print(f"""\
        # DEBUG SPIN KEYS:
        # yaw0      = {yaw0:.4f} rad
        # yaw1      = {yaw1:.4f} rad
        # dθ         = {dθ:.4f} rad  (should be yaw1–yaw0 in (–π,π])
        # total_ang = {total_ang:.4f} rad (|Δθ|+2π)
        # theta_end = {theta_end:.4f} rad
        # theta_end mod2π = {theta_end % (2*np.pi):.4f} rad
        # yaw1     mod2π = {yaw1 % (2*np.pi):.4f} rad
        # """)

        # 4) build the spin keyframe config
        cfg = th.generate_spin_keyframes(
            name=f"loiter_spin_{t0:.2f}",
            Nco=Nco,
            xyz=pos_rrt,
            theta0=yaw0,
            theta1=yaw1,
            time=spin_duration
        )

        if idx == 0 and course_cfg_path is not None:
            import os, json
            # ensure the directory exists
            os.makedirs(course_cfg_path, exist_ok=True)
            fname = f"{cfg['name'].replace(' ', '_')}.json"
            out_path = os.path.join(course_cfg_path, fname)
            # write (overwrites if already there)
            with open(out_path, "w") as f:
                json.dump(cfg, f, indent=2)
            # print(f"Wrote spin config to {out_path}")

        # 5) solve it
        out = ms.solve(cfg)

        if out is False:
            raise RuntimeError(f"Spin QP failed at t0={t0:.2f}")
        Tps, CPs = out
        tXUd_spin = th.TS_to_tXU(Tps, CPs, base_frame_specs, hz)

        if idx == 0 and course_cfg_path is not None:
            # ensure the directory exists (in case it wasn’t created yet)
            os.makedirs(course_cfg_path, exist_ok=True)

            # JSON was:   /.../<course_cfg_path>/<cfg['name']>.json
            # let’s put the pickle in the same dir:
            pkl_fname = f"loiter_tXUd_{t0:.2f}.pkl"
            pkl_path  = os.path.join(course_cfg_path, pkl_fname)

            with open(pkl_path, "wb") as f:
                pickle.dump(tXUd_spin, f)

            # print(f"Wrote spin rollout to {pkl_path}")

#FIXME
        if simulate:
            tXUd_full = th.build_loiter_fragment(
                tXUd_spin=tXUd_spin,
                tXUd_rrt=tXUd_rrt,
                t0=t0,
                smooth_duration=smooth_duration,
                tail_duration=tXUd_rrt[0, -1] - t0,
                hz=hz
            )

            loiter_trajs.append(tXUd_full)
        else:
            tXUd_full = th.build_loiter_fragment(
                tXUd_spin=tXUd_spin,
                tXUd_rrt=tXUd_rrt,
                t0=t0,
                smooth_duration=smooth_duration,
                tail_duration=3.0,
                hz=hz
            )

            loiter_trajs.append(tXUd_full)

    return loiter_trajs

def generate_perturbations(Tsps:np.ndarray|None=None,
                           Tpd:np.ndarray|None=None,
                           tXUi:np.ndarray|None=None,
                           CPd:np.ndarray|None=None,
                           trajectory_set_config:Dict[str,Union[int,bool]]|None=None,
                           rng_seed:int=None) -> List[Dict[str,Union[float,np.ndarray]]]:
    """
    Generates a list of perturbed initial states for the drone given an ideal trajectory. The perturbed
    initial states are generated by sampling a random initial times and corresponding state vectors from
    the ideal trajectory using a bounded uniform distribution. The state vectors are then perturbed with
    uniform noise. The number of perturbed initial states generated is determined by the sample set
    config dictionary.

    Args:
        Tsps:                   Sample times.
        Tpd:                    Ideal trajectory times.
        CPi:                    Ideal trajectory control points.
        tXUi:                   Ideal trajectory in state space format.
        trajectory_set_config:  Trajectory set config dictionary.
        rng_seed:               Random number generator seed.

    Returns:
        Perturbations:          List of perturbations (dictionary format).
    """

    # Sample Count
    Nsps = len(Tsps)

    # Set random number generator seed
    if rng_seed is not None:
        np.random.seed(rng_seed)
    
    # Unpack the config
    w_x0 = np.array(trajectory_set_config["initial"],dtype=float)
    
    # Get ideal trajectory for quaternion checking
    if Tpd is not None:
        tXUd = th.TS_to_tXU(Tpd,CPd,None,10)
    elif tXUi is not None:
        tXUd = copy.deepcopy(tXUi)
        Tpd = tXUd[0]

    # Generate perturbed starting points    
    Perturbations = []
    for i in range(Nsps):
        # Sample random start time and get corresponding state vector sample
        t0 = Tsps[i]
        idx0 = np.where(Tpd <= t0)[0][-1]
        idx0 = min(idx0,(len(Tpd)-2))
        t00,t0f = Tpd[idx0],Tpd[idx0 + 1]

        if CPd is not None:
            x0s = th.ts_to_xu(t0-t00,t0f-t00,CPd[idx0,:,:],None)
        else:
            x0s = tXUd[1:11,idx0]
        
        # Perturb state vector sample
        w0 = np.random.uniform(w_x0[0,:],w_x0[1,:])
        x0 = x0s + w0
        
        # Ensure quaternion is well-behaved (magnitude and closest to previous)
        idxr = np.where(tXUd[0,:] <= t0)[0][-1]
        x0[6:10] = th.obedient_quaternion(x0[6:10],tXUd[7:11,idxr])

        # Store perturbation in list
        perturbation = {"t0":t0,"x0":x0}
        Perturbations.append(perturbation)
    
    return Perturbations

def generate_rollouts(
        sim:Simulator,
        sample_config,
        tXUd:np.ndarray|None=None,
        course_config:Dict[str,Union[np.ndarray,List[np.ndarray]]]|None=None,
        objective:List[str]|None=None,
        policy_config:Dict[str,Union[int,float,List[float]]]|None=None,
        Frames:Dict[str,Union[np.ndarray,str,int,float]]|None=None,
        Perturbations:Dict[str,Union[float,np.ndarray]]|None=None,
        Tdt_ro:float|None=None,err_tol:float|None=None,
        vision_processor:bool=False,
        validation_mode:bool=False
        ) -> Tuple[List[Dict[str,Union[np.ndarray,np.ndarray,np.ndarray]]],List[torch.Tensor]]:
    """
    Generates rollout data for the quadcopter given a list of drones and initial states (perturbations).
    The rollout comprises trajectory data and image data. The trajectory data is generated by running
    the MPC controller on the quadcopter for a fixed number of steps. The trajectory data consists of
    time, states [p,v,q], body rate inputs [fn,w], objective state, data count, solver timings, advisor
    data, rollout id, and course name. The image data is generated by rendering the quadcopter at each
    state in the trajectory data. The image data consists of the image data and the data count.

#TODO   Args:
        simulator:          Simulator object.
        sample_config:      Sample set config dictionary.
        course_config:      Course configuration dictionary.
        policy_config:      Policy configuration dictionary.
        Frames:             List of drone configurations.
        Perturbations:      List of perturbed initial states.
        Tdt_ro:             Rollout duration.

    Returns:
        Trajectories:           List of trajectory rollouts.
        Images:                 List of image rollouts.
    """
    
    rrt_mode = sample_config["rrt_mode"]

    if rrt_mode:
        # Unpack sample set config
        # mu_md = np.array(sample_config["model_noise"]["mean"])
        # std_md = np.array(sample_config["model_noise"]["std"])
        # mu_sn = np.array(sample_config["sensor_noise"]["mean"])
        # std_sn = np.array(sample_config["sensor_noise"]["std"])
        # hz_ctl = sample_config["simulation"]["hz_ctl"]
        # hz_sim = sample_config["simulation"]["hz_sim"]
        # t_dly = sample_config["simulation"]["delay"]
        # dt_ro = sample_config["rollout_duration"]
        videoMode = sample_config["videoMode"]

        # Initialize rollout variables
        Trajectories, Images, Image_Data = [], [], []

        obj = np.zeros((18,1))

        rolling_idx = 0
        # Rollout the trajectories
        for idx, (frame_config, perturbation) in enumerate(zip(Frames, Perturbations)):
            # Unpack rollout variables
            t0, x0 = perturbation["t0"], perturbation["x0"]

#NOTE Validation specific behavior
            if validation_mode:
                Tdt_ro = tXUd[0,-1] - t0

            tf = t0 + Tdt_ro

            # Load the simulation variables
            sim.load_frame(frame_config)
            ctl = VehicleRateMPC(tXUd, policy_config, frame_config)

            # Simulate the flight
            Tro, Xro, Uro, Imgs, Tsol, Adv = sim.simulate(
                ctl, t0, tf, x0,
                query=objective,
                clipseg=vision_processor,
                validation=validation_mode
            )

            trajectory = {
                "Tro": Tro,
                "Xro": Xro,
                "Uro": Uro,
                "Xid": tXUd[1:, :],
                "obj": obj,
                "Ndata": Uro.shape[1],
                "Tsol": Tsol,
                "Adv": Adv,
                "rollout_id": str(idx).zfill(5),
                "course": objective,
                "frame": frame_config
            }

            # compute frame offsets for video sequences
            if videoMode:
                start_idx = rolling_idx
                end_idx = rolling_idx + len(Imgs["semantic"]) - 1
                rolling_idx = end_idx + 1

                Image_Data.append({
                    "rollout_id": str(idx).zfill(5),
                    "start_id": start_idx,
                    "end_id": end_idx
                })

            # Build image entry depending on validation mode
            if validation_mode:
                has_roll = "validation" in Imgs
                if has_roll:
                    entry = {
                        "rollout_id": str(idx).zfill(5),
                        "course": objective,
                        "val_images": Imgs.get("semantic", []),
                        "val_rollout_images": Imgs.get("validation", [])
                    }
                else:
                    entry = {
                        "rollout_id": str(idx).zfill(5),
                        "course": objective,
                        "val_images": Imgs.get("semantic", []),
                    }
            else:
                entry = {
                    "rollout_id": str(idx).zfill(5),
                    "course": objective,
                    "images": Imgs.get("semantic", [])
                }

            Images.append(entry)
            Trajectories.append(trajectory)

            del ctl

        return Trajectories, Images, Image_Data
    else:
        # Unpack the trajectory
        Tpi,CPi = ms.solve(course_config)
        obj = sh.ts_to_obj(Tpi,CPi)
        tXUd = th.TS_to_tXU(Tpi,CPi,None,10)
        
        # Initialize rollout variables
        Trajectories,Images = [],[]

        # Rollout the trajectories
        for idx,(frame_config,perturbation) in enumerate(zip(Frames,Perturbations)):
            # Unpack rollout variables
            t0,x0 = perturbation["t0"],perturbation["x0"]
            tf = t0 + Tdt_ro

            # Load the simulation variables
            sim.load_frame(frame_config)
            ctl = VehicleRateMPC(course_config,policy_config,frame_config)
            
            # Simulate the flight
            Tro,Xro,Uro,Imgs,Tsol,Adv = sim.simulate(ctl,t0,tf,x0)
            
            # Check if the rollout data is useful
            err = np.min(np.linalg.norm(tXUd[1:4,:]-Xro[0:3,-1].reshape(-1,1),axis=0))
            if err < err_tol:
                # Package the rollout data
                trajectory = {
                    "Tro":Tro,"Xro":Xro,"Uro":Uro,
                    "tXUd":tXUd,"obj":obj,"Ndata":Uro.shape[1],"Tsol":Tsol,"Adv":Adv,
                    "rollout_id":str(idx).zfill(5),
                    "course":course_config["name"],
                    "frame":frame_config}

                images = {
                    "images":Imgs,
                    "rollout_id":str(idx).zfill(5),"course":course_config["name"]
                }

                # Store rollout data
                Trajectories.append(trajectory)
                Images.append(images)

            # Clear policy
            del ctl

        return Trajectories,Images

def save_rollouts(
    cohort_path: str,
    course_name: str,
    Trajectories: List[Dict],
    Images: List[Dict],
    Image_Data,
    tXUd: np.ndarray,
    stack_id: Union[str, int],
    validation_mode: bool = False
) -> None:
    """
    Saves rollout data to .pt files and converts images to videos,
    preserving the original file-name patterns.
    """
    # Ensure output directory exists
    rollout_course_path = os.path.join(cohort_path, "rollout_data", course_name)
    os.makedirs(rollout_course_path, exist_ok=True)

    # Format the dataset suffix
    ds = str(stack_id).zfill(5) if isinstance(stack_id, int) else str(stack_id)

    # Determine file paths exactly as in original code
    if validation_mode:
        trajectory_data_set_path    = os.path.join(rollout_course_path, f"trajectories_val{ds}.pt")
        image_data_set_path         = os.path.join(rollout_course_path, f"imgdata_val{ds}.pt")
        video_data_set_path         = os.path.join(rollout_course_path, f"video_val{ds}.mp4")
        video_rollout_data_set_path = os.path.join(rollout_course_path, f"video_val_rollout{ds}.mp4")
    else:
        trajectory_data_set_path = os.path.join(rollout_course_path, f"trajectories{ds}.pt")
        image_data_set_path      = os.path.join(rollout_course_path, f"imgdata{ds}.pt")
        video_data_set_path      = os.path.join(rollout_course_path, f"video{ds}.mp4")

    # Save trajectory data
    Ndata = sum(traj["Ndata"] for traj in Trajectories)
    trajectory_data_set = {
        "data":   Trajectories,
        "tXUd":   tXUd,
        "set":    ds,
        "Ndata":  Ndata,
        "course": course_name
    }
    torch.save(trajectory_data_set, trajectory_data_set_path)

    # Flatten and save videos
    if validation_mode:
        # validation frames
        all_val   = [f for entry in Images for f in entry.get("val_images", [])]
        du.save_images_as_video(all_val, video_data_set_path)

        has_roll = any("val_rollout_images" in entry for entry in Images)
        if has_roll:
            all_roll = [f for entry in Images for f in entry.get("val_rollout_images", [])]
            du.save_images_as_video(all_roll, video_rollout_data_set_path)

        image_data_set = {
            "data":          Image_Data,
            "set":           video_data_set_path,
            "Ndata":         Ndata,
            "course":        course_name
        }
    else:
        all_frames = [f for entry in Images for f in entry.get("images", [])]
        du.save_images_as_video(all_frames, video_data_set_path)

        image_data_set = {
            "data":     Image_Data,
            "set":      video_data_set_path,
            "Ndata":    Ndata,
            "course":   course_name
        }

    # Save image metadata
    torch.save(image_data_set, image_data_set_path)